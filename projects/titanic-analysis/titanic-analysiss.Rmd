---
title: "Titanic Analysis"
author: "Resma Yunita"
date: "15/09/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This analysis attempts to predicate the probability for survival of the Titanic passengers. In order to do this, I will use the different features available about the passengers, use a subset of the data to train an algorithm and then run the algorithm on the rest of the data set to get a prediction.

In this analysis I asked the following questions:

1.  What is the relationship the features and a passenger’s chance of survival.

2.  Prediction of survival for the entire ship.

# Data loading and cleaning

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(ggplot2)
library(dplyr)
library(GGally)
library(rpart)
library(rpart.plot)
library(randomForest)

# I loaded the data:

test <- read.csv('E:/datamining/titanic_prediction/test.csv',stringsAsFactors = FALSE)
train <- read.csv('E:/datamining/titanic_prediction/train.csv', stringsAsFactors = FALSE)

# Creating a new data set with both the test and the train sets
full <- bind_rows(train,test)
LT=dim(train)[1]
# Checking the structure
str(full)
```

```{r}
# Missing values
colSums(is.na(full))
colSums(full=="")
# We have a lot of missing data in the Age feature (263/1309)
# Let's change the empty strings in Embarked to the first choice "C"
full$Embarked[full$Embarked==""]="C"

# Let's see how many features we can move to factors
apply(full,2, function(x) length(unique(x)))

# Let's move the features Survived, Pclass, Sex, Embarked to be factors
cols<-c("Survived","Pclass","Sex","Embarked")
for (i in cols){
  full[,i] <- as.factor(full[,i])
}

# Now lets look on the structure of the full data set
str(full)
```

# Analysis

At this time point I loaded and cleaned a little bit of the data. Now it’s time to look at the relationships between the different features:

```{r}
# First, let's look at the relationship between sex and survival:
ggplot(data=full[1:LT,],aes(x=Sex,fill=Survived))+geom_bar()
# Survival as a function of Embarked:
ggplot(data = full[1:LT,],aes(x=Embarked,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")
```

```{r}
t<-table(full[1:LT,]$Embarked,full[1:LT,]$Survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
t
```

```{r}
#It looks that you have a better chance to survive if you Embarked in 'C' (55% compared to 33% and 38%).

# Survival as a function of Pclass:
ggplot(data = full[1:LT,],aes(x=Pclass,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")
```

```{r}
# It looks like you have a better chance to survive if you in lower ticket class.
# Now, let's devide the graph of Embarked by Pclass:
ggplot(data = full[1:LT,],aes(x=Embarked,fill=Survived))+geom_bar(position="fill")+facet_wrap(~Pclass)
```

```{r}
# Now it's not so clear that there is a correlation between Embarked and Survival. 

# Survivial as a function of SibSp and Parch
ggplot(data = full[1:LT,],aes(x=SibSp,fill=Survived))+geom_bar()
```

```{r}
ggplot(data = full[1:LT,],aes(x=Parch,fill=Survived))+geom_bar()
```

```{r warning=FALSE}
# The dymanics of SibSp and Parch are very close one each other.
# Let's try to look at another parameter: family size.

full$FamilySize <- full$SibSp + full$Parch +1;
full1<-full[1:LT,]
ggplot(data = full1[!is.na(full[1:LT,]$FamilySize),],aes(x=FamilySize,fill=Survived))+geom_histogram(binwidth =1,position="fill")+ylab("Frequency")
```

```{r}
# That shows that families with a family size bigger or equal to 2 but less than 6 have a more than 50% to survive, in contrast to families with 1 member or more than 5 members. 

# Survival as a function of age:

ggplot(data = full1[!(is.na(full[1:LT,]$Age)),],aes(x=Age,fill=Survived))+geom_histogram(binwidth =3)
```

```{r warning=FALSE}
ggplot(data = full1[!is.na(full[1:LT,]$Age),],aes(x=Age,fill=Survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frequency")
```

```{r warning=FALSE}
# Children (younger than 15YO) and old people (80 and up) had a better chance to survive.

# Is there a correlation between Fare and Survivial?
ggplot(data = full[1:LT,],aes(x=Fare,fill=Survived))+geom_histogram(binwidth =20, position="fill")
```

```{r}
full$Fare[is.na(full$Fare)] <- mean(full$Fare,na.rm=T)
# It seems like if your fare is bigger, than you have a better chance to survive.
sum(is.na(full$Age))
```

```{r}
# There are a lot of missing values in the Age feature, so I'll put the mean instead of the missing values.
full$Age[is.na(full$Age)] <- mean(full$Age,na.rm=T)
sum(is.na(full$Age))
```

```{r}
# The title of the passanger can affect his survive:
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
full$Title[full$Title == 'Mlle']<- 'Miss' 
full$Title[full$Title == 'Ms']<- 'Miss'
full$Title[full$Title == 'Mme']<- 'Mrs' 
full$Title[full$Title == 'Lady']<- 'Miss'
full$Title[full$Title == 'Dona']<- 'Miss'
officer<- c('Capt','Col','Don','Dr','Jonkheer','Major','Rev','Sir','the Countess')
full$Title[full$Title %in% officer]<-'Officer'

full$Title<- as.factor(full$Title)

ggplot(data = full[1:LT,],aes(x=Title,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")
```

# Prediction

At this time point we want to predict the chance of survival as a function of the other features. I'm going to keep just the correlated features: Pclass, Sex, Age, SibSp, Parch, Title and Fare. I'm going to divide the train set into two sets: training set (train1) and test set (train2) to be able to estimate the error of the prediction.

```{r echo=TRUE,message=FALSE,warning=FALSE}
# The train set with the important features 
train_im<- full[1:LT,c("Survived","Pclass","Sex","Age","Fare","SibSp","Parch","Title")]
ind<-sample(1:dim(train_im)[1],500) # Sample of 500 out of 891
train1<-train_im[ind,] # The train set of the model
train2<-train_im[-ind,] # The test set of the model

# Let's try to run a logistic regression
model <- glm(Survived ~.,family=binomial(link='logit'),data=train1)
summary(model)
```

```{r  warning=FALSE}
# We can see that SibSp, Parch and Fare are not statisticaly significant. 
# Let's look at the prediction of this model on the test set (train2):
pred.train <- predict(model,train2)
pred.train <- ifelse(pred.train > 0.5,1,0)
# Mean of the true prediction 
mean(pred.train==train2$Survived)
```

```{r}
t1<-table(pred.train,train2$Survived)
# Presicion and recall of the model
presicion<- t1[1,1]/(sum(t1[1,]))
recall<- t1[1,1]/(sum(t1[,1]))
presicion
```

```{r}
recall
```

```{r}
# F1 score
F1<- 2*presicion*recall/(presicion+recall)
F1
```

```{r}
# F1 score on the initial test set is 0.871. This pretty good.

# Let's run it on the test set:

test_im<-full[LT+1:1309,c("Pclass","Sex","Age","SibSp","Parch","Fare","Title")]

pred.test <- predict(model,test_im)[1:418]
pred.test <- ifelse(pred.test > 0.5,1,0)
res<- data.frame(test$PassengerId,pred.test)
names(res)<-c("PassengerId","Survived")
write.csv(res,file="res.csv",row.names = F)
```

At this time point I want to predict survival using a decision tree model:

```{r}
model_dt<- rpart(Survived ~.,data=train1, method="class")
rpart.plot(model_dt)
```

```{r}
pred.train.dt <- predict(model_dt,train2,type = "class")
mean(pred.train.dt==train2$Survived)
```

```{r}
t2<-table(pred.train.dt,train2$Survived)

presicion_dt<- t2[1,1]/(sum(t2[1,]))
recall_dt<- t2[1,1]/(sum(t2[,1]))
presicion_dt
```

```{r}
recall_dt
```

```{r}
F1_dt<- 2*presicion_dt*recall_dt/(presicion_dt+recall_dt)
F1_dt
```

```{r}
# Let's run this model on the test set:
pred.test.dt <- predict(model_dt,test_im,type="class")[1:418]
res_dt<- data.frame(test$PassengerId,pred.test.dt)
names(res_dt)<-c("PassengerId","Survived")
write.csv(res_dt,file="res_dt.csv",row.names = F)

# Let's try to predict survival using a random forest.
model_rf<-randomForest(Survived~.,data=train1)
# Let's look at the error
plot(model_rf)
```

```{r}
pred.train.rf <- predict(model_rf,train2)
mean(pred.train.rf==train2$Survived)
```

```{r}
t1<-table(pred.train.rf,train2$Survived)
presicion<- t1[1,1]/(sum(t1[1,]))
recall<- t1[1,1]/(sum(t1[,1]))
presicion

```

```{r}
recall
```

```{r}
F1<- 2*presicion*recall/(presicion+recall)
F1
```

```{r}
# Let's run this model on the test set:
pred.test.rf <- predict(model_rf,test_im)[1:418]
res_rf<- data.frame(test$PassengerId,pred.test.rf)
names(res_rf)<-c("PassengerId","Survived")
write.csv(res_rf,file="res_rf.csv",row.names = F)
```

# Conclusion

In this analysis of the Titanic survival dataset, several models were employed to predict passenger survival based on various features. The main findings are summarized as follows:

1.  **Data Insights**:

    -   **Embarkation Point**: Passengers who embarked from port 'C' had a significantly higher survival rate (55%) compared to those from 'Q' (33%) and 'S' (38%).

    -   **Passenger Class**: Survival rates decreased with higher ticket class; first-class passengers had better chances than those in second or third class.

    -   **Family Size**: Passengers with family sizes between 2 and 5 had over a 50% chance of survival, while those traveling alone or with larger families fared worse.

    -   **Age and Fare**: Younger passengers (especially children) and older adults (80+) showed higher survival rates. Additionally, higher fares were correlated with better survival chances.

2.  **Model Performance**:

    -   **Logistic Regression**: Achieved an F1 score of 0.8588 on the validation set, indicating good predictive performance.

    -   **Decision Tree**: Resulted in an F1 score of 0.8376, showing reasonable accuracy but slightly less than logistic regression.

    -   **Random Forest**: This model outperformed others with an F1 score of 0.8473, demonstrating its robustness in handling the feature interactions.

3.  **Overall Predictions**:

    -   The random forest model provided the best predictions on the test dataset, yielding a survival prediction accuracy of approximately 80.8%.
